{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7072b8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fasttext-numpy2-wheel\n",
      "  Downloading fasttext_numpy2_wheel-0.9.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
      "Requirement already satisfied: huggingface_hub in /home/bjrc/anaconda3/lib/python3.12/site-packages (0.36.0)\n",
      "Collecting pybind11>=2.2 (from fasttext-numpy2-wheel)\n",
      "  Downloading pybind11-3.0.1-py3-none-any.whl.metadata (10.0 kB)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in /home/bjrc/anaconda3/lib/python3.12/site-packages (from fasttext-numpy2-wheel) (75.1.0)\n",
      "Collecting numpy>=2 (from fasttext-numpy2-wheel)\n",
      "  Downloading numpy-2.3.5-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: filelock in /home/bjrc/anaconda3/lib/python3.12/site-packages (from huggingface_hub) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/bjrc/anaconda3/lib/python3.12/site-packages (from huggingface_hub) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/bjrc/anaconda3/lib/python3.12/site-packages (from huggingface_hub) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/bjrc/anaconda3/lib/python3.12/site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in /home/bjrc/anaconda3/lib/python3.12/site-packages (from huggingface_hub) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/bjrc/anaconda3/lib/python3.12/site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/bjrc/anaconda3/lib/python3.12/site-packages (from huggingface_hub) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/bjrc/anaconda3/lib/python3.12/site-packages (from huggingface_hub) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/bjrc/anaconda3/lib/python3.12/site-packages (from requests->huggingface_hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/bjrc/anaconda3/lib/python3.12/site-packages (from requests->huggingface_hub) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/bjrc/anaconda3/lib/python3.12/site-packages (from requests->huggingface_hub) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/bjrc/anaconda3/lib/python3.12/site-packages (from requests->huggingface_hub) (2025.10.5)\n",
      "Downloading fasttext_numpy2_wheel-0.9.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.3.5-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pybind11-3.0.1-py3-none-any.whl (293 kB)\n",
      "Installing collected packages: pybind11, numpy, fasttext-numpy2-wheel\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.3.5 which is incompatible.\n",
      "tianshou 1.2.0 requires numpy<2,>=1, but you have numpy 2.3.5 which is incompatible.\n",
      "numba 0.61.2 requires numpy<2.3,>=1.24, but you have numpy 2.3.5 which is incompatible.\n",
      "scipy 1.13.1 requires numpy<2.3,>=1.22.4, but you have numpy 2.3.5 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed fasttext-numpy2-wheel-0.9.2 numpy-2.3.5 pybind11-3.0.1\n"
     ]
    }
   ],
   "source": [
    "# À lancer une seule fois si fasttext / huggingface_hub ne sont pas installés\n",
    "!pip install fasttext-numpy2-wheel huggingface_hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea170a46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb8d4ae773b8457e9077e29e54bed800",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_v1.bin:   0%|          | 0.00/1.38G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from: /home/bjrc/.cache/huggingface/hub/models--cis-lmu--glotlid/snapshots/74cb50b709c9eefe0f790030c6c95c461b4e3b77/model_v1.bin\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "import fasttext\n",
    "\n",
    "# Télécharger le modèle GlotLID-M (v3)\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"cis-lmu/glotlid\",\n",
    "    filename=\"model_v1.bin\",   # ou \"model_v1.bin\" si tu veux la version de l’article\n",
    "    cache_dir=None\n",
    ")\n",
    "\n",
    "print(\"Model loaded from:\", model_path)\n",
    "\n",
    "# Charger le modèle fastText\n",
    "model = fasttext.load_model(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5179de45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def predict_top1(sentence: str, threshold: float = 0.5):\n",
    "    \"\"\"\n",
    "    Retourne :\n",
    "    - label GlotLID (ex: 'fra_Latn', 'eng_Latn', 'wol_Latn', ...)\n",
    "    - probabilité associée\n",
    "    - ou ('undetermined', prob) si probabilité < threshold\n",
    "    \"\"\"\n",
    "    labels, probs = model.predict(sentence, k=1)\n",
    "    label = labels[0].replace(\"__label__\", \"\")\n",
    "    prob = float(probs[0])\n",
    "    if prob < threshold:\n",
    "        return \"undetermined\", prob\n",
    "    return label, prob\n",
    "\n",
    "\n",
    "def predict_topk(sentence: str, k: int = 3):\n",
    "    \"\"\"\n",
    "    Retourne les k meilleures langues avec leurs probabilités,\n",
    "    sans appliquer de seuil.\n",
    "    \"\"\"\n",
    "    labels, probs = model.predict(sentence, k=k)\n",
    "    labels = [lab.replace(\"__label__\", \"\") for lab in labels]\n",
    "    probs = [float(p) for p in probs]\n",
    "    return list(zip(labels, probs))\n",
    "\n",
    "\n",
    "def print_prediction(sentence: str, threshold: float = 0.5, k: int = 3):\n",
    "    \"\"\"\n",
    "    Affiche :\n",
    "    - phrase\n",
    "    - prédiction top-1 avec seuil\n",
    "    - top-k sans seuil (pour visualiser les langues cousines)\n",
    "    \"\"\"\n",
    "    top1_label, top1_prob = predict_top1(sentence, threshold=threshold)\n",
    "    topk = predict_topk(sentence, k=k)\n",
    "\n",
    "    print(\"________________________________________\")\n",
    "    print(f\"Texte : {sentence}\")\n",
    "    print(f\"Top-1 (θ={threshold}) : {top1_label} (p={top1_prob:.3f})\")\n",
    "    print(\"Top-k without threshold:\")\n",
    "    for lab, p in topk:\n",
    "        print(f\"   - {lab:10s}  p={p:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e37a36f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "interesting_examples = [\n",
    "    # Simple French\n",
    "    \"Bonjour, nous somme des étudiants en intelligence artificielle à Paris.\",\n",
    "    \n",
    "    # English\n",
    "    \"We are working on a low-resource language identification project using GlotLID.\",\n",
    "    \n",
    "    # Spanish\n",
    "    \"Este es un texto en español sobre procesamiento del lenguaje natural.\",\n",
    "    \n",
    "    # Standard Arabic\n",
    "    \"الذكاء الاصطناعي أصبح مجالاً مهماً في السنوات الأخيرة.\",\n",
    "    \n",
    "    # Russian\n",
    "    \"Машинное обучение и обработка естественного языка тесно связаны.\",\n",
    "    \n",
    "    # Chinese (Simplified)\n",
    "    \"人工智能和机器学习在语言处理方面非常有用。\",\n",
    "    \n",
    "    # Wolof (African low-resource language)\n",
    "    \"Nanga def? Maa ngi fi rekk, jërëjëf.\",\n",
    "    \n",
    "    # Hausa (another African language commonly found in web corpora)\n",
    "    \"Ina son koyon kimiyyar kwamfuta da harsunan wucin gadi.\",\n",
    "    \n",
    "    # Quechua / Indigenous American language (depending on the model: quz_Latn / others)\n",
    "    \"Qusqu llaqtapi runasimita yachayku.\",\n",
    "    \n",
    "    # Very short ambiguous phrase (could be FR, EN, ES, etc.)\n",
    "    \"Merci.\",\n",
    "    \n",
    "    # Another very short phrase: 'OK' is possible in many languages\n",
    "    \"OK\",\n",
    "    \n",
    "    # Code-switch French + English\n",
    "    \"Franchement ce paper sur GlotLID is really impressive.\",\n",
    "    \n",
    "    # Code-switch Spanish + English\n",
    "    \"Este modelo funciona muy bien on web data.\",\n",
    "    \n",
    "    # Only digits / symbols -> should be uncertain or low confidence\n",
    "    \"12345 !!! $$$\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a56c3f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________\n",
      "Texte : Bonjour, nous somme des étudiants en intelligence artificielle à Paris.\n",
      "Top-1 (θ=0.5) : fra (p=0.999)\n",
      "Top-k sans seuil :\n",
      "   - fra         p=0.999\n",
      "   - pcd         p=0.000\n",
      "   - dhv         p=0.000\n",
      "   - gxx         p=0.000\n",
      "   - aba         p=0.000\n",
      "________________________________________\n",
      "Texte : We are working on a low-resource language identification project using GlotLID.\n",
      "Top-1 (θ=0.5) : eng (p=0.991)\n",
      "Top-k sans seuil :\n",
      "   - eng         p=0.991\n",
      "   - por         p=0.001\n",
      "   - rus         p=0.001\n",
      "   - ilo         p=0.001\n",
      "   - swh         p=0.001\n",
      "________________________________________\n",
      "Texte : Este es un texto en español sobre procesamiento del lenguaje natural.\n",
      "Top-1 (θ=0.5) : spa (p=0.995)\n",
      "Top-k sans seuil :\n",
      "   - spa         p=0.995\n",
      "   - grn         p=0.004\n",
      "   - quz         p=0.000\n",
      "   - cat         p=0.000\n",
      "   - glg         p=0.000\n",
      "________________________________________\n",
      "Texte : الذكاء الاصطناعي أصبح مجالاً مهماً في السنوات الأخيرة.\n",
      "Top-1 (θ=0.5) : ara (p=0.593)\n",
      "Top-k sans seuil :\n",
      "   - ara         p=0.593\n",
      "   - arb         p=0.388\n",
      "   - ary         p=0.017\n",
      "   - arz         p=0.001\n",
      "   - ars         p=0.000\n",
      "________________________________________\n",
      "Texte : Машинное обучение и обработка естественного языка тесно связаны.\n",
      "Top-1 (θ=0.5) : rus (p=0.999)\n",
      "Top-k sans seuil :\n",
      "   - rus         p=0.999\n",
      "   - rml         p=0.000\n",
      "   - kjh         p=0.000\n",
      "   - sah         p=0.000\n",
      "   - oss         p=0.000\n",
      "________________________________________\n",
      "Texte : 人工智能和机器学习在语言处理方面非常有用。\n",
      "Top-1 (θ=0.5) : zho (p=0.958)\n",
      "Top-k sans seuil :\n",
      "   - zho         p=0.958\n",
      "   - cmn         p=0.042\n",
      "   - wuu         p=0.000\n",
      "   - lzh         p=0.000\n",
      "   - yue         p=0.000\n",
      "________________________________________\n",
      "Texte : Nanga def? Maa ngi fi rekk, jërëjëf.\n",
      "Top-1 (θ=0.5) : wol (p=0.999)\n",
      "Top-k sans seuil :\n",
      "   - wol         p=0.999\n",
      "   - men         p=0.000\n",
      "   - gaz         p=0.000\n",
      "   - orm         p=0.000\n",
      "   - sqi         p=0.000\n",
      "________________________________________\n",
      "Texte : Ina son koyon kimiyyar kwamfuta da harsunan wucin gadi.\n",
      "Top-1 (θ=0.5) : hau (p=1.000)\n",
      "Top-k sans seuil :\n",
      "   - hau         p=1.000\n",
      "   - yal         p=0.000\n",
      "   - knc         p=0.000\n",
      "   - aai         p=0.000\n",
      "   - taq         p=0.000\n",
      "________________________________________\n",
      "Texte : Qusqu llaqtapi runasimita yachayku.\n",
      "Top-1 (θ=0.5) : que (p=1.000)\n",
      "Top-k sans seuil :\n",
      "   - que         p=1.000\n",
      "   - quh         p=0.000\n",
      "   - quz         p=0.000\n",
      "   - quy         p=0.000\n",
      "   - qul         p=0.000\n",
      "________________________________________\n",
      "Texte : Merci.\n",
      "Top-1 (θ=0.5) : fra (p=0.935)\n",
      "Top-k sans seuil :\n",
      "   - fra         p=0.935\n",
      "   - sag         p=0.036\n",
      "   - deu         p=0.016\n",
      "   - gsw         p=0.007\n",
      "   - pcd         p=0.002\n",
      "________________________________________\n",
      "Texte : OK\n",
      "Top-1 (θ=0.5) : ces (p=0.984)\n",
      "Top-k sans seuil :\n",
      "   - ces         p=0.984\n",
      "   - lit         p=0.014\n",
      "   - hun         p=0.001\n",
      "   - nld         p=0.000\n",
      "   - pol         p=0.000\n",
      "________________________________________\n",
      "Texte : Franchement ce paper sur GlotLID is really impressive.\n",
      "Top-1 (θ=0.5) : eng (p=0.980)\n",
      "Top-k sans seuil :\n",
      "   - eng         p=0.980\n",
      "   - fra         p=0.008\n",
      "   - nld         p=0.001\n",
      "   - lir         p=0.001\n",
      "   - mlt         p=0.001\n",
      "________________________________________\n",
      "Texte : Este modelo funciona muy bien on web data.\n",
      "Top-1 (θ=0.5) : spa (p=0.999)\n",
      "Top-k sans seuil :\n",
      "   - spa         p=0.999\n",
      "   - por         p=0.001\n",
      "   - grn         p=0.000\n",
      "   - lad         p=0.000\n",
      "   - ron         p=0.000\n",
      "________________________________________\n",
      "Texte : 12345 !!! $$$\n",
      "Top-1 (θ=0.5) : eng (p=0.991)\n",
      "Top-k sans seuil :\n",
      "   - eng         p=0.991\n",
      "   - twi         p=0.002\n",
      "   - fao         p=0.002\n",
      "   - cor         p=0.001\n",
      "   - nno         p=0.000\n"
     ]
    }
   ],
   "source": [
    "theta = 0.5  # seuil de confiance, comme dans l’article\n",
    "\n",
    "for sent in interesting_examples:\n",
    "    print_prediction(sent, threshold=theta, k=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22d1fd80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target language: wol   |   confidence threshold θ = 0.5\n",
      "\n",
      "Raw mixed corpus with GlotLID-M predictions:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_c68c7\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_c68c7_level0_col0\" class=\"col_heading level0 col0\" >text</th>\n",
       "      <th id=\"T_c68c7_level0_col1\" class=\"col_heading level0 col1\" >pred_lang</th>\n",
       "      <th id=\"T_c68c7_level0_col2\" class=\"col_heading level0 col2\" >conf</th>\n",
       "      <th id=\"T_c68c7_level0_col3\" class=\"col_heading level0 col3\" >keep_for_corpus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_c68c7_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_c68c7_row0_col0\" class=\"data row0 col0\" >Nanga def? Maa ngi fi rekk, jërëjëf.</td>\n",
       "      <td id=\"T_c68c7_row0_col1\" class=\"data row0 col1\" >wol</td>\n",
       "      <td id=\"T_c68c7_row0_col2\" class=\"data row0 col2\" >0.999</td>\n",
       "      <td id=\"T_c68c7_row0_col3\" class=\"data row0 col3\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c68c7_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_c68c7_row1_col0\" class=\"data row1 col0\" >Jërëjëf waay, ba beneen yoon.</td>\n",
       "      <td id=\"T_c68c7_row1_col1\" class=\"data row1 col1\" >wol</td>\n",
       "      <td id=\"T_c68c7_row1_col2\" class=\"data row1 col2\" >1.000</td>\n",
       "      <td id=\"T_c68c7_row1_col3\" class=\"data row1 col3\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c68c7_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_c68c7_row2_col0\" class=\"data row2 col0\" >Bonjour, ceci est un texte en français mais mal étiqueté comme wolof dans le corpus.</td>\n",
       "      <td id=\"T_c68c7_row2_col1\" class=\"data row2 col1\" >fra</td>\n",
       "      <td id=\"T_c68c7_row2_col2\" class=\"data row2 col2\" >1.000</td>\n",
       "      <td id=\"T_c68c7_row2_col3\" class=\"data row2 col3\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c68c7_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_c68c7_row3_col0\" class=\"data row3 col0\" >We accidentally mixed some English sentences into this 'wolof' dataset.</td>\n",
       "      <td id=\"T_c68c7_row3_col1\" class=\"data row3 col1\" >eng</td>\n",
       "      <td id=\"T_c68c7_row3_col2\" class=\"data row3 col2\" >0.998</td>\n",
       "      <td id=\"T_c68c7_row3_col3\" class=\"data row3 col3\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c68c7_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_c68c7_row4_col0\" class=\"data row4 col0\" >Ina son koyon kimiyyar kwamfuta da harsunan wucin gadi.</td>\n",
       "      <td id=\"T_c68c7_row4_col1\" class=\"data row4 col1\" >hau</td>\n",
       "      <td id=\"T_c68c7_row4_col2\" class=\"data row4 col2\" >1.000</td>\n",
       "      <td id=\"T_c68c7_row4_col3\" class=\"data row4 col3\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c68c7_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_c68c7_row5_col0\" class=\"data row5 col0\" >Nanga def, sama xale yi jang nañu l'anglais ak le français.</td>\n",
       "      <td id=\"T_c68c7_row5_col1\" class=\"data row5 col1\" >wol</td>\n",
       "      <td id=\"T_c68c7_row5_col2\" class=\"data row5 col2\" >1.000</td>\n",
       "      <td id=\"T_c68c7_row5_col3\" class=\"data row5 col3\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c68c7_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_c68c7_row6_col0\" class=\"data row6 col0\" >Este es un texto en español que no debería estar en un corpus de wolof.</td>\n",
       "      <td id=\"T_c68c7_row6_col1\" class=\"data row6 col1\" >spa</td>\n",
       "      <td id=\"T_c68c7_row6_col2\" class=\"data row6 col2\" >0.972</td>\n",
       "      <td id=\"T_c68c7_row6_col3\" class=\"data row6 col3\" >False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f42d0f53b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "➡ Cleaned corpus (only high-confidence sentences in target language):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_ce2f1\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_ce2f1_level0_col0\" class=\"col_heading level0 col0\" >text</th>\n",
       "      <th id=\"T_ce2f1_level0_col1\" class=\"col_heading level0 col1\" >conf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_ce2f1_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_ce2f1_row0_col0\" class=\"data row0 col0\" >Nanga def? Maa ngi fi rekk, jërëjëf.</td>\n",
       "      <td id=\"T_ce2f1_row0_col1\" class=\"data row0 col1\" >0.999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ce2f1_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_ce2f1_row1_col0\" class=\"data row1 col0\" >Jërëjëf waay, ba beneen yoon.</td>\n",
       "      <td id=\"T_ce2f1_row1_col1\" class=\"data row1 col1\" >1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ce2f1_level0_row2\" class=\"row_heading level0 row2\" >5</th>\n",
       "      <td id=\"T_ce2f1_row2_col0\" class=\"data row2 col0\" >Nanga def, sama xale yi jang nañu l'anglais ak le français.</td>\n",
       "      <td id=\"T_ce2f1_row2_col1\" class=\"data row2 col1\" >1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f41c9115fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Kept 3 / 7 sentences for the wol corpus.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# === Parameters for the demo ===\n",
    "target_lang = \"wol\"   # low-resource-ish language (Wolof)\n",
    "theta = 0.5                # confidence threshold, as in the paper examples\n",
    "\n",
    "# === Fake \"raw\" corpus: supposed to be Wolof, but actually mixed/noisy ===\n",
    "raw_corpus = [\n",
    "    # True Wolof sentences\n",
    "    \"Nanga def? Maa ngi fi rekk, jërëjëf.\",\n",
    "    \"Jërëjëf waay, ba beneen yoon.\",\n",
    "    \n",
    "    # French sentence wrongly stored in a 'wolof' file\n",
    "    \"Bonjour, ceci est un texte en français mais mal étiqueté comme wolof dans le corpus.\",\n",
    "    \n",
    "    # English sentence mixed in the same file\n",
    "    \"We accidentally mixed some English sentences into this 'wolof' dataset.\",\n",
    "    \n",
    "    # Hausa sentence (another African language, should be filtered out)\n",
    "    \"Ina son koyon kimiyyar kwamfuta da harsunan wucin gadi.\",\n",
    "    \n",
    "    # Wolof + French code-switch (often realistic on social media)\n",
    "    \"Nanga def, sama xale yi jang nañu l'anglais ak le français.\",\n",
    "    \n",
    "    # Spanish sentence that should not appear in a Wolof corpus\n",
    "    \"Este es un texto en español que no debería estar en un corpus de wolof.\"\n",
    "]\n",
    "\n",
    "def classify_and_filter(corpus, target_lang: str, threshold: float):\n",
    "    \"\"\"\n",
    "    For each sentence in corpus:\n",
    "    - run GlotLID-M\n",
    "    - keep only sentences predicted as target_lang with confidence >= threshold\n",
    "    Returns a pandas DataFrame with columns:\n",
    "      text, pred_lang, conf, keep_for_corpus\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for sent in corpus:\n",
    "        labels, probs = model.predict(sent, k=1)\n",
    "        label = labels[0].replace(\"__label__\", \"\")\n",
    "        prob = float(probs[0])\n",
    "        keep = (label == target_lang) and (prob >= threshold)\n",
    "        rows.append({\n",
    "            \"text\": sent,\n",
    "            \"pred_lang\": label,\n",
    "            \"conf\": prob,\n",
    "            \"keep_for_corpus\": keep\n",
    "        })\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df\n",
    "\n",
    "# Run the cleaning pipeline\n",
    "df = classify_and_filter(raw_corpus, target_lang, theta)\n",
    "\n",
    "print(f\"Target language: {target_lang}   |   confidence threshold θ = {theta}\")\n",
    "print(\"\\nRaw mixed corpus with GlotLID-M predictions:\")\n",
    "display(df.style.format({\"conf\": \"{:.3f}\"}))\n",
    "\n",
    "# Filter to obtain the \"clean\" low-resource corpus\n",
    "clean_df = df[df[\"keep_for_corpus\"]]\n",
    "\n",
    "print(\"\\n➡ Cleaned corpus (only high-confidence sentences in target language):\")\n",
    "display(clean_df[[\"text\", \"conf\"]].style.format({\"conf\": \"{:.3f}\"}))\n",
    "print(f\"\\nKept {len(clean_df)} / {len(df)} sentences for the {target_lang} corpus.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9279ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
