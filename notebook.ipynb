{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "656db13c",
   "metadata": {},
   "source": [
    "# GlotLID and MaskLID Experiments\n",
    "\n",
    "This notebook sets up GlotLID for sentence-level LID and runs MaskLID for code-switching on a small sample dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d3945a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete.\n"
     ]
    }
   ],
   "source": [
    "# Environment setup: install required packages\n",
    "import sys, subprocess\n",
    "\n",
    "def pip_install(pkg):\n",
    "    print(f\"Installing {pkg}...\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg])\n",
    "\n",
    "# Install huggingface_hub and numpy if missing\n",
    "try:\n",
    "    import huggingface_hub  # noqa: F401\n",
    "except Exception:\n",
    "    pip_install(\"huggingface_hub\")\n",
    "\n",
    "try:\n",
    "    import numpy as np  # noqa: F401\n",
    "except Exception:\n",
    "    pip_install(\"numpy\")\n",
    "\n",
    "# Install fasttext (Windows-friendly): prefer fasttext-numpy2-wheel, else fallback to fasttext\n",
    "fasttext = None\n",
    "try:\n",
    "    import fasttext  # type: ignore\n",
    "except Exception:\n",
    "    try:\n",
    "        pip_install(\"fasttext-numpy2-wheel\")\n",
    "        import fasttext  # type: ignore  # noqa: E402\n",
    "    except Exception:\n",
    "        pip_install(\"fasttext\")\n",
    "        import fasttext  # type: ignore  # noqa: E402\n",
    "\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "912c7818",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jmigu\\anaconda3\\envs\\glotlid310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model path: C:\\Users\\jmigu\\.cache\\huggingface\\hub\\models--cis-lmu--glotlid\\snapshots\\74cb50b709c9eefe0f790030c6c95c461b4e3b77\\model.bin\n",
      "Loaded GlotLID with 2102 labels\n",
      "(('__label__eng_Latn', '__label__isl_Latn', '__label__deu_Latn'), array([9.9636394e-01, 2.0239984e-03, 5.2679953e-04], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "# Download and load GlotLID model\n",
    "from huggingface_hub import hf_hub_download\n",
    "import fasttext\n",
    "import numpy as np\n",
    "\n",
    "# Download latest model (v3 as of README); you can pin to model_v3.bin\n",
    "model_path = hf_hub_download(repo_id=\"cis-lmu/glotlid\", filename=\"model.bin\", cache_dir=None)\n",
    "print(\"Model path:\", model_path)\n",
    "\n",
    "# Load the model\n",
    "model = fasttext.load_model(model_path)\n",
    "print(\"Loaded GlotLID with\", len(model.labels), \"labels\")\n",
    "\n",
    "# Custom predict using output_matrix + softmax (avoids numpy 2.x issue in fasttext.predict)\n",
    "labels = model.get_labels()\n",
    "output_matrix = model.get_output_matrix()\n",
    "\n",
    "def _softmax(x: np.ndarray) -> np.ndarray:\n",
    "    x = np.asarray(x)\n",
    "    e = np.exp(x - np.max(x))\n",
    "    return e / e.sum()\n",
    "\n",
    "def glotlid_predict(text: str, k: int = 3):\n",
    "    sv = model.get_sentence_vector(text)\n",
    "    logits = np.dot(output_matrix, sv)\n",
    "    probs = _softmax(logits)\n",
    "    top_idx = np.argsort(probs)[-k:][::-1]\n",
    "    top_labels = tuple(labels[i] for i in top_idx)\n",
    "    top_probs = probs[top_idx]\n",
    "    return top_labels, top_probs\n",
    "\n",
    "# Quick sanity check\n",
    "print(glotlid_predict(\"Hello, world!\", k=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48d204c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT: Hello, how are you?\n",
      "PRED: ('__label__eng_Latn', '__label__ind_Latn', '__label__sna_Latn') [0.9999676942825317, 3.187024049111642e-05, 2.1771765545963717e-07]\n",
      "-\n",
      "TEXT: ¿Cómo estás? Todo bien.\n",
      "PRED: ('__label__spa_Latn', '__label__glg_Latn', '__label__gug_Latn') [0.9999990463256836, 7.620928954565898e-07, 1.3862313608115073e-07]\n",
      "-\n",
      "TEXT: Merhaba, nasılsın?\n",
      "PRED: ('__label__tur_Latn', '__label__azj_Latn', '__label__diq_Latn') [0.9999998807907104, 1.2680378347340593e-07, 4.7710493333852355e-08]\n",
      "-\n",
      "TEXT: C'est une belle journée.\n",
      "PRED: ('__label__fra_Latn', '__label__oci_Latn', '__label__fro_Latn') [1.0, 1.8759225284270542e-08, 1.1848276137982339e-08]\n",
      "-\n"
     ]
    }
   ],
   "source": [
    "# Helper: GlotLID predictions on a small dataset\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "def predict_topk(texts: List[str], k: int = 3) -> List[Tuple[tuple, list]]:\n",
    "    results = []\n",
    "    for t in texts:\n",
    "        labels_, probs_ = glotlid_predict(t, k)\n",
    "        results.append((labels_, probs_.tolist()))\n",
    "    return results\n",
    "\n",
    "examples = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"¿Cómo estás? Todo bien.\",\n",
    "    \"Merhaba, nasılsın?\",\n",
    "    \"C'est une belle journée.\",\n",
    "]\n",
    "\n",
    "glotlid_results = predict_topk(examples, k=3)\n",
    "for text, (labels_, probs_) in zip(examples, glotlid_results):\n",
    "    print(\"TEXT:\", text)\n",
    "    print(\"PRED:\", labels_, probs_)\n",
    "    print(\"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c74d51f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__label__tur_Latn: 0.9994\n",
      "__label__azj_Latn: 0.0003\n",
      "__label__kiu_Latn: 0.0001\n",
      "__label__gag_Latn: 0.0001\n",
      "__label__crh_Latn: 0.0001\n",
      "__label__kaa_Latn: 0.0000\n",
      "__label__diq_Latn: 0.0000\n",
      "__label__tuk_Latn: 0.0000\n",
      "__label__tat_Latn: 0.0000\n",
      "__label__uig_Latn: 0.0000\n",
      "__label__kmr_Latn: 0.0000\n",
      "__label__dgr_Latn: 0.0000\n",
      "__label__kas_Latn: 0.0000\n",
      "__label__rhg_Latn: 0.0000\n",
      "__label__daa_Latn: 0.0000\n"
     ]
    }
   ],
   "source": [
    "def show_top_k(text: str, k: int = 3):\n",
    "    labels_, probs_ = glotlid_predict(text, k)\n",
    "    for label, prob in zip(labels_, probs_):\n",
    "        print(f\"{label}: {prob:.4f}\")\n",
    "\n",
    "example_text = \"bir kahve dükkanında geçen film tadında güzel bir şarkıya ayrılsın gece falling in love at a coffee shop\"\n",
    "\n",
    "show_top_k(example_text, k=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2466fbf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code-switch text:\n",
      " bir kahve dükkanında geçen film tadında güzel bir şarkıya ayrılsın gece falling in love at a coffee shop\n",
      "MaskLID segments:\n",
      "__label__tur_Latn : bir kahve dükkanında geçen tadında güzel bir şarkıya ayrılsın gece\n",
      "\n",
      "Masked text:\n",
      " [MASK] [MASK] [MASK] [MASK] film [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] falling in love at a coffee shop\n",
      "\n",
      "Remaining (unassigned):\n",
      " film falling in love at a coffee shop\n"
     ]
    }
   ],
   "source": [
    "# MaskLID: Code-switching experiments (refactored into reusable pipeline)\n",
    "# Import MaskLID from local repo folder\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import re, string\n",
    "\n",
    "root = Path.cwd()\n",
    "sys.path.insert(0, str(root / \"MaskLID\"))\n",
    "from masklid import MaskLID  # noqa: E402\n",
    "\n",
    "# Reuse GlotLID model file\n",
    "masklid_model = MaskLID(str(model_path), languages=-1)\n",
    "\n",
    "# --- Small helpers ---\n",
    "\n",
    "def normalize_for_mask(text: str) -> str:\n",
    "    \"\"\"Normalize text the same way MaskLID does for consistent tokenization.\"\"\"\n",
    "    replace_by = \" \"\n",
    "    replacement_map = {ord(c): replace_by for c in \"\\n_:\" + \"•#{|}\" + string.digits}\n",
    "    text = text.translate(replacement_map)\n",
    "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "def build_assigned_counter(segments: dict) -> Counter:\n",
    "    \"\"\"Build a multiset (Counter) of all tokens assigned to some language.\"\"\"\n",
    "    assigned = []\n",
    "    for seg in segments.values():\n",
    "        assigned.extend(seg.split())\n",
    "    return Counter(assigned)\n",
    "\n",
    "def apply_mask_to_tokens(tokens: list[str], assigned: Counter, mask_token: str = \"[MASK]\") -> tuple[list[str], list[str]]:\n",
    "    \"\"\"Return masked tokens and remaining tokens by consuming from the assigned multiset.\"\"\"\n",
    "    masked_tokens: list[str] = []\n",
    "    remaining_tokens: list[str] = []\n",
    "    for tok in tokens:\n",
    "        if assigned.get(tok, 0) > 0:\n",
    "            assigned[tok] -= 1\n",
    "            masked_tokens.append(mask_token)\n",
    "        else:\n",
    "            masked_tokens.append(tok)\n",
    "            remaining_tokens.append(tok)\n",
    "    return masked_tokens, remaining_tokens\n",
    "\n",
    "def apply_mask_and_remaining(text: str, segments: dict, mask_token: str = \"[MASK]\") -> tuple[str, str]:\n",
    "    \"\"\"Normalize text, compute assigned-token multiset, and produce masked/remaining strings.\"\"\"\n",
    "    tokens = normalize_for_mask(text).split()\n",
    "    assigned = build_assigned_counter(segments)\n",
    "    masked_tokens, remaining_tokens = apply_mask_to_tokens(tokens, assigned, mask_token)\n",
    "    return \" \".join(masked_tokens), \" \".join(remaining_tokens)\n",
    "\n",
    "# --- Global pipeline function ---\n",
    "\n",
    "def run_masklid_pipeline(\n",
    "    text: str,\n",
    "    model: MaskLID,\n",
    "    *,\n",
    "    beta: int = 20,\n",
    "    alpha: int = 3,\n",
    "    max_lambda: int = 3,\n",
    "    min_length: int = 10,\n",
    "    min_prob: float = 0.90,\n",
    "    max_retry: int = 3,\n",
    "    alpha_step_increase: int = 3,\n",
    "    beta_step_increase: int = 5,\n",
    "    mask_token: str = \"[MASK]\",\n",
    ") -> dict:\n",
    "    \"\"\"Run MaskLID, then apply a token-level mask and return everything for reuse.\"\"\"\n",
    "    segments = model.predict_codeswitch(\n",
    "        text,\n",
    "        beta=beta,\n",
    "        alpha=alpha,\n",
    "        max_lambda=max_lambda,\n",
    "        min_length=min_length,\n",
    "        min_prob=min_prob,\n",
    "        max_retry=max_retry,\n",
    "        alpha_step_increase=alpha_step_increase,\n",
    "        beta_step_increase=beta_step_increase,\n",
    "    )\n",
    "    masked_text, remaining_text = apply_mask_and_remaining(text, segments, mask_token)\n",
    "    return {\n",
    "        \"segments\": segments,\n",
    "        \"masked_text\": masked_text,\n",
    "        \"remaining_text\": remaining_text,\n",
    "    }\n",
    "\n",
    "# --- Example usage ---\n",
    "# Pick the code-switching example (from earlier)\n",
    "cs_text = example_text\n",
    "print(\"Code-switch text:\\n\", cs_text)\n",
    "\n",
    "res = run_masklid_pipeline(\n",
    "    cs_text,\n",
    "    masklid_model,\n",
    "    beta=20,\n",
    "    alpha=3,\n",
    "    max_lambda=3,\n",
    "    min_length=10,\n",
    "    min_prob=0.90,\n",
    "    max_retry=3,\n",
    "    alpha_step_increase=3,\n",
    "    beta_step_increase=5,\n",
    "    mask_token=\"[MASK]\",\n",
    ")\n",
    "\n",
    "print(\"MaskLID segments:\")\n",
    "for lang, seg in res[\"segments\"].items():\n",
    "    print(lang, \":\", seg)\n",
    "\n",
    "print(\"\\nMasked text:\\n\", res[\"masked_text\"])\n",
    "print(\"\\nRemaining (unassigned):\\n\", res[\"remaining_text\"]) \n",
    "\n",
    "# Expose for downstream cells\n",
    "masked_text = res[\"masked_text\"]\n",
    "remaining_text = res[\"remaining_text\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fd04619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__label__eng_Latn: 0.9999\n",
      "__label__enm_Latn: 0.0001\n",
      "__label__srd_Latn: 0.0000\n",
      "__label__jam_Latn: 0.0000\n",
      "__label__ita_Latn: 0.0000\n",
      "__label__kin_Latn: 0.0000\n",
      "__label__guj_Latn: 0.0000\n",
      "__label__hau_Latn: 0.0000\n",
      "__label__ind_Latn: 0.0000\n",
      "__label__pap_Latn: 0.0000\n"
     ]
    }
   ],
   "source": [
    "show_top_k(remaining_text, k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62115d8c",
   "metadata": {},
   "source": [
    "It works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c0dc172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__label__por_Latn: 0.9913\n",
      "__label__spa_Latn: 0.0047\n",
      "__label__eng_Latn: 0.0018\n",
      "SEGMENTS FROM MASKLID:\n",
      "{'__label__por_Latn': 'Vamos Benfica!'}\n",
      "\n",
      "Masked text:\n",
      " [MASK] [MASK] Let's go\n",
      "Remaining (unassigned):\n",
      " Let's go\n",
      "__label__eng_Latn: 0.9995\n",
      "__label__fur_Latn: 0.0003\n",
      "__label__nso_Latn: 0.0002\n",
      "__label__deu_Latn: 0.0001\n",
      "__label__srd_Latn: 0.0000\n",
      "__label__vec_Latn: 0.0000\n",
      "__label__tsn_Latn: 0.0000\n",
      "__label__dan_Latn: 0.0000\n",
      "__label__gle_Latn: 0.0000\n",
      "__label__und_Batk: 0.0000\n"
     ]
    }
   ],
   "source": [
    "new_example = \"Vamos Benfica! Let's go\"\n",
    "\n",
    "show_top_k(new_example, k=3)\n",
    "\n",
    "res = run_masklid_pipeline(\n",
    "    new_example,\n",
    "    masklid_model,\n",
    "    beta=20,\n",
    "    alpha=3,\n",
    "    max_lambda=3,\n",
    "    min_length=5,\n",
    "    min_prob=0.90,\n",
    "    max_retry=3,\n",
    "    alpha_step_increase=3,\n",
    "    beta_step_increase=5,\n",
    "    mask_token=\"[MASK]\",\n",
    ")\n",
    "\n",
    "# DEBUG: Check what segments MaskLID returned\n",
    "print(\"SEGMENTS FROM MASKLID:\")\n",
    "print(res[\"segments\"])\n",
    "print()\n",
    "\n",
    "print(\"Masked text:\\n\", res[\"masked_text\"])\n",
    "print(\"Remaining (unassigned):\\n\", res[\"remaining_text\"])\n",
    "\n",
    "show_top_k(res[\"remaining_text\"], k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9957a029",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glotlid310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
